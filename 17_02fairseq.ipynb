{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17-02fairseq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KKRpehdqxMtn0SX7j62h-8NlIFfXEeC9",
      "authorship_tag": "ABX9TyOZfvfT0WrmIhnxCBpFSZTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meghraj-Webllisto/language-translation/blob/master/17_02fairseq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mCRKAqzPSey",
        "colab_type": "text"
      },
      "source": [
        "# working on prepairing training data fo coustom language chinese or japense or korean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyrOZH3GPdTQ",
        "colab_type": "code",
        "outputId": "be1e93a1-69d8-4480-867d-cf9e41d1c00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/fairseq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fairseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK1abx8rPocZ",
        "colab_type": "code",
        "outputId": "94e7b94f-857e-4f91-e18a-6f9bd0c2c2b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/              \u001b[01;34mfairseq\u001b[0m/           interactive.py            \u001b[01;34mscripts\u001b[0m/\n",
            "CODE_OF_CONDUCT.md  \u001b[01;34mfairseq_cli\u001b[0m/       \u001b[01;34miwslt17.de_fr.en.bpe16k\u001b[0m/  setup.py\n",
            "CONTRIBUTING.md     \u001b[01;34mfairseq.egg-info\u001b[0m/  \u001b[01;34miwslt17_orig\u001b[0m/             \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdist\u001b[0m/               fairseq.gif        LICENSE                   train.py\n",
            "\u001b[01;34mdocs\u001b[0m/               fairseq_logo.png   preprocess.py             validate.py\n",
            "eval_lm.py          generate.py        README.md\n",
            "\u001b[01;34mexamples\u001b[0m/           hubconf.py         score.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMrm2vNYPpak",
        "colab_type": "code",
        "outputId": "d9b7ccd9-bc98-4425-ce87-017da03df2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd scripts "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fairseq/scripts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4C6ThI9PsyW",
        "colab_type": "code",
        "outputId": "322df5c6-761e-4f86-fdf4-58eee177053f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average_checkpoints.py  count_docs.py        split_train_valid_docs.py\n",
            "build_sym_alignment.py  __init__.py          spm_decode.py\n",
            "compare_namespaces.py   read_binarized.py    spm_encode.py\n",
            "compound_split_bleu.sh  rm_pt.py             spm_train.py\n",
            "convert_dictionary.lua  sacrebleu_pregen.sh  wav2vec_featurize.py\n",
            "convert_model.lua       shard_docs.py        wav2vec_manifest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQaTAfB7Pt97",
        "colab_type": "code",
        "outputId": "0cedb490-35b1-4ae1-e126-032ac9d51fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd .."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fairseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWmyhG5PUal1",
        "colab_type": "code",
        "outputId": "353ab5be-1b53-486e-e1ae-77203eab550f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RipaNQUnXxCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f5ac5ae-a363-49e3-d0fb-c03c4bf4c798"
      },
      "source": [
        "rm -r custom_orig"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'custom_orig': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnE9LhaDKOXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT = en\n",
        "Lang = en\n",
        "for LANG in \"ja\" \"en\"; do\n",
        "    cat \"$ORIG/ja\"-${TGT}.${LANG} \\\n",
        "        | grep -v '<url>' \\\n",
        "        | grep -v '<talkid>' \\\n",
        "        | grep -v '<keywords>' \\\n",
        "        | grep -v '<speaker>' \\\n",
        "        | grep -v '<reviewer' \\\n",
        "        | grep -v '<translator' \\\n",
        "        | grep -v '<doc' \\\n",
        "        | grep -v '</doc>' \\\n",
        "        | sed -e 's/<title>//g' \\\n",
        "        | sed -e 's/<\\/title>//g' \\\n",
        "        | sed -e 's/<description>//g' \\\n",
        "        | sed -e 's/<\\/description>//g' \\\n",
        "        | sed 's/^\\s*//g' \\\n",
        "        | sed 's/\\s*$//g' \\\n",
        "        > \"$DATA/${SRC}-${TGT}.${LANG}\"\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryQIC8IqMgKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90079dfc-b5be-4e2c-a820-46777e29bc32"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/fairseq'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOlq_gm8P1ef",
        "colab_type": "code",
        "outputId": "cfefbf04-aeae-4da8-fe89-39f7e5ad6ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "%%bash\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "SRCS=(\n",
        "    \"en\"\n",
        ")\n",
        "TGT=zh\n",
        "\n",
        "ROOT=$(dirname \"$0\")\n",
        "SCRIPTS=./scripts\n",
        "SPM_TRAIN=$SCRIPTS/spm_train.py\n",
        "SPM_ENCODE=$SCRIPTS/spm_encode.py\n",
        "\n",
        "BPESIZE=16384\n",
        "ORIG=$ROOT/custom_orig\n",
        "DATA=$ROOT/custom.en-zh.en.bpe16k\n",
        "mkdir -p \"$ORIG\" \"$DATA\"\n",
        "\n",
        "TRAIN_MINLEN=1  # remove sentences with <1 BPE token\n",
        "TRAIN_MAXLEN=250  # remove sentences with >250 BPE tokens\n",
        "\n",
        "# URLS=(\n",
        "#     \"https://wit3.fbk.eu/archive/2017-01-trnted/texts/de/en/de-en.tgz\"\n",
        "\n",
        "#     \"https://wit3.fbk.eu/archive/2017-01-trnted/texts/fr/en/fr-en.tgz\"\n",
        "# )\n",
        "# ARCHIVES=(\n",
        "#     \"de-en.tgz\"\n",
        "#     \"fr-en.tgz\"\n",
        "# )\n",
        "# VALID_SETS=(\n",
        "#     \"IWSLT17.TED.dev2010.de-en IWSLT17.TED.tst2010.de-en IWSLT17.TED.tst2011.de-en IWSLT17.TED.tst2012.de-en IWSLT17.TED.tst2013.de-en IWSLT17.TED.tst2014.de-en IWSLT17.TED.tst2015.de-en\"\n",
        "#     \"IWSLT17.TED.dev2010.fr-en IWSLT17.TED.tst2010.fr-en IWSLT17.TED.tst2011.fr-en IWSLT17.TED.tst2012.fr-en IWSLT17.TED.tst2013.fr-en IWSLT17.TED.tst2014.fr-en IWSLT17.TED.tst2015.fr-en\"\n",
        "# )\n",
        "\n",
        "# download and extract data\n",
        "# for ((i=0;i<${#URLS[@]};++i)); do\n",
        "#     ARCHIVE=$ORIG/${ARCHIVES[i]}\n",
        "#     if [ -f \"$ARCHIVE\" ]; then\n",
        "#         echo \"$ARCHIVE already exists, skipping download\"\n",
        "#     else\n",
        "#         URL=${URLS[i]}\n",
        "#         wget -P \"$ORIG\" \"$URL\"\n",
        "#         if [ -f \"$ARCHIVE\" ]; then\n",
        "#             echo \"$URL successfully downloaded.\"\n",
        "#         else\n",
        "#             echo \"$URL not successfully downloaded.\"\n",
        "#             exit 1\n",
        "#         fi\n",
        "#     fi\n",
        "#     FILE=${ARCHIVE: -4}\n",
        "#     if [ -e \"$FILE\" ]; then\n",
        "#         echo \"$FILE already exists, skipping extraction\"\n",
        "#     else\n",
        "#         tar -C \"$ORIG\" -xzvf \"$ARCHIVE\"\n",
        "#     fi\n",
        "# done\n",
        "\n",
        "echo \"pre-processing train data...\"\n",
        "\n",
        "for LANG in \"zh\" \"en\"; do\n",
        "    cat \"$ORIG/en\"-${TGT}.${LANG} \\\n",
        "        | grep -v '<url>' \\\n",
        "        | grep -v '<talkid>' \\\n",
        "        | grep -v '<keywords>' \\\n",
        "        | grep -v '<speaker>' \\\n",
        "        | grep -v '<reviewer' \\\n",
        "        | grep -v '<translator' \\\n",
        "        | grep -v '<doc' \\\n",
        "        | grep -v '</doc>' \\\n",
        "        | sed -e 's/<title>//g' \\\n",
        "        | sed -e 's/<\\/title>//g' \\\n",
        "        | sed -e 's/<description>//g' \\\n",
        "        | sed -e 's/<\\/description>//g' \\\n",
        "        | sed 's/^\\s*//g' \\\n",
        "        | sed 's/\\s*$//g' \\\n",
        "        > \"$DATA/train.en${SRC}-${TGT}.${LANG}\"\n",
        "done\n",
        "\n",
        "# echo \"pre-processing valid data...\"\n",
        "# for ((i=0;i<${#SRCS[@]};++i)); do\n",
        "#     SRC=${SRCS[i]}\n",
        "#     VALID_SET=(${VALID_SETS[i]})\n",
        "#     for ((j=0;j<${#VALID_SET[@]};++j)); do\n",
        "#         FILE=${VALID_SET[j]}\n",
        "#         for LANG in \"$SRC\" \"$TGT\"; do\n",
        "#             grep '<seg id' \"$ORIG/${SRC}-${TGT}/${FILE}.${LANG}.xml\" \\\n",
        "#                 | sed -e 's/<seg id=\"[0-9]*\">\\s*//g' \\\n",
        "#                 | sed -e 's/\\s*<\\/seg>\\s*//g' \\\n",
        "#                 | sed -e \"s/\\’/\\'/g\" \\\n",
        "#                 > \"$DATA/valid${j}.${SRC}-${TGT}.${LANG}\"\n",
        "#         done\n",
        "#     done\n",
        "# done\n",
        "\n",
        "# learn BPE with sentencepiece\n",
        "# TRAIN_FILES=$(for SRC in \"${SRCS[@]}\"; do \\\n",
        "# echo $DATA/train.${SRC}-${TGT}.${SRC}; \\\n",
        "# echo $DATA/train.${SRC}-${TGT}.${TGT}; done | tr \"\\n\" \",\")\n",
        "# echo \"learning joint BPE over ${TRAIN_FILES}...\"\n",
        "# python \"$SPM_TRAIN\" \\\n",
        "#     --input=$TRAIN_FILES \\\n",
        "#     --model_prefix=$DATA/sentencepiece.bpe \\\n",
        "#     --vocab_size=$BPESIZE \\\n",
        "#     --character_coverage=1.0 \\\n",
        "#     --model_type=bpe\n",
        "\n",
        "# encode train/valid/test\n",
        "echo \"encoding train/valid with learned BPE...\"\n",
        "for SRC in \"${SRCS[@]}\"; do\n",
        "    for LANG in \"$SRC\" \"$TGT\"; do\n",
        "        python \"$SPM_ENCODE\" \\\n",
        "            --model \"$DATA/sentencepiece.bpe.model\" \\\n",
        "            --output_format=piece \\\n",
        "            --inputs $DATA/train.${SRC}-${TGT}.${SRC} $DATA/train.${SRC}-${TGT}.${TGT} \\\n",
        "            --outputs $DATA/train.bpe.${SRC}-${TGT}.${SRC} $DATA/train.bpe.${SRC}-${TGT}.${TGT} \\\n",
        "            --min-len $TRAIN_MINLEN --max-len $TRAIN_MAXLEN\n",
        "        # python \"$SPM_ENCODE\" \\\n",
        "        #     --model \"$DATA/sentencepiece.bpe.model\" \\\n",
        "        #     --output_format=piece \\\n",
        "        #     --inputs $DATA/valid.${SRC}-${TGT}.${SRC} $DATA/valid.${SRC}-${TGT}.${TGT} \\\n",
        "        #     --outputs $DATA/valid.bpe.${SRC}-${TGT}.${SRC} $DATA/valid.bpe.${SRC}-${TGT}.${TGT}\n",
        "    done\n",
        "done"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre-processing train data...\n",
            "encoding train/valid with learned BPE...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "processed 10000 lines\n",
            "processed 20000 lines\n",
            "processed 30000 lines\n",
            "processed 40000 lines\n",
            "processed 50000 lines\n",
            "processed 60000 lines\n",
            "processed 70000 lines\n",
            "skipped 49 empty lines\n",
            "filtered 0 lines\n",
            "processed 10000 lines\n",
            "processed 20000 lines\n",
            "processed 30000 lines\n",
            "processed 40000 lines\n",
            "processed 50000 lines\n",
            "processed 60000 lines\n",
            "processed 70000 lines\n",
            "skipped 49 empty lines\n",
            "filtered 0 lines\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfS0qRAmDNZB",
        "colab_type": "code",
        "outputId": "305b69a9-5d5d-468c-cfc5-fa4d23e4cbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sentencepiece as s\n",
        "print(s.__file__)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sentencepiece.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeE5eZXYTvbq",
        "colab_type": "code",
        "outputId": "96a71ae6-8651-4f23-bcc4-a8717cbf97cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%bash\n",
        "#!/bin/bash\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "SRCS=(\n",
        "    \"de\"\n",
        "    \"fr\"\n",
        ")\n",
        "TGT=en\n",
        "\n",
        "ROOT=$(dirname \"$0\")\n",
        "SCRIPTS=$ROOT/scripts\n",
        "SPM_TRAIN=$SCRIPTS/spm_train.py\n",
        "SPM_ENCODE=$SCRIPTS/spm_encode.py\n",
        "\n",
        "BPESIZE=16384\n",
        "ORIG=$ROOT/iwslt17_orig\n",
        "DATA=$ROOT/iwslt17.de_fr.en.bpe16k\n",
        "mkdir -p \"$ORIG\" \"$DATA\"\n",
        "\n",
        "TRAIN_MINLEN=1  # remove sentences with <1 BPE token\n",
        "TRAIN_MAXLEN=250  # remove sentences with >250 BPE tokens\n",
        "\n",
        "URLS=(\n",
        "    \"https://wit3.fbk.eu/archive/2017-01-trnted/texts/de/en/de-en.tgz\"\n",
        "\n",
        "    \"https://wit3.fbk.eu/archive/2017-01-trnted/texts/fr/en/fr-en.tgz\"\n",
        ")\n",
        "ARCHIVES=(\n",
        "    \"de-en.tgz\"\n",
        "    \"fr-en.tgz\"\n",
        ")\n",
        "VALID_SETS=(\n",
        "    \"IWSLT17.TED.dev2010.de-en IWSLT17.TED.tst2010.de-en IWSLT17.TED.tst2011.de-en IWSLT17.TED.tst2012.de-en IWSLT17.TED.tst2013.de-en IWSLT17.TED.tst2014.de-en IWSLT17.TED.tst2015.de-en\"\n",
        "    \"IWSLT17.TED.dev2010.fr-en IWSLT17.TED.tst2010.fr-en IWSLT17.TED.tst2011.fr-en IWSLT17.TED.tst2012.fr-en IWSLT17.TED.tst2013.fr-en IWSLT17.TED.tst2014.fr-en IWSLT17.TED.tst2015.fr-en\"\n",
        ")\n",
        "\n",
        "# download and extract data\n",
        "for ((i=0;i<${#URLS[@]};++i)); do\n",
        "    ARCHIVE=$ORIG/${ARCHIVES[i]}\n",
        "    if [ -f \"$ARCHIVE\" ]; then\n",
        "        echo \"$ARCHIVE already exists, skipping download\"\n",
        "    else\n",
        "        URL=${URLS[i]}\n",
        "        wget -P \"$ORIG\" \"$URL\"\n",
        "        if [ -f \"$ARCHIVE\" ]; then\n",
        "            echo \"$URL successfully downloaded.\"\n",
        "        else\n",
        "            echo \"$URL not successfully downloaded.\"\n",
        "            exit 1\n",
        "        fi\n",
        "    fi\n",
        "    FILE=${ARCHIVE: -4}\n",
        "    if [ -e \"$FILE\" ]; then\n",
        "        echo \"$FILE already exists, skipping extraction\"\n",
        "    else\n",
        "        tar -C \"$ORIG\" -xzvf \"$ARCHIVE\"\n",
        "    fi\n",
        "done\n",
        "\n",
        "echo \"pre-processing train data...\"\n",
        "for SRC in \"${SRCS[@]}\"; do\n",
        "    for LANG in \"${SRC}\" \"${TGT}\"; do\n",
        "        cat \"$ORIG/${SRC}-${TGT}/train.tags.${SRC}-${TGT}.${LANG}\" \\\n",
        "            | grep -v '<url>' \\\n",
        "            | grep -v '<talkid>' \\\n",
        "            | grep -v '<keywords>' \\\n",
        "            | grep -v '<speaker>' \\\n",
        "            | grep -v '<reviewer' \\\n",
        "            | grep -v '<translator' \\\n",
        "            | grep -v '<doc' \\\n",
        "            | grep -v '</doc>' \\\n",
        "            | sed -e 's/<title>//g' \\\n",
        "            | sed -e 's/<\\/title>//g' \\\n",
        "            | sed -e 's/<description>//g' \\\n",
        "            | sed -e 's/<\\/description>//g' \\\n",
        "            | sed 's/^\\s*//g' \\\n",
        "            | sed 's/\\s*$//g' \\\n",
        "            > \"$DATA/train.${SRC}-${TGT}.${LANG}\"\n",
        "    done\n",
        "done\n",
        "\n",
        "echo \"pre-processing valid data...\"\n",
        "for ((i=0;i<${#SRCS[@]};++i)); do\n",
        "    SRC=${SRCS[i]}\n",
        "    VALID_SET=(${VALID_SETS[i]})\n",
        "    for ((j=0;j<${#VALID_SET[@]};++j)); do\n",
        "        FILE=${VALID_SET[j]}\n",
        "        for LANG in \"$SRC\" \"$TGT\"; do\n",
        "            grep '<seg id' \"$ORIG/${SRC}-${TGT}/${FILE}.${LANG}.xml\" \\\n",
        "                | sed -e 's/<seg id=\"[0-9]*\">\\s*//g' \\\n",
        "                | sed -e 's/\\s*<\\/seg>\\s*//g' \\\n",
        "                | sed -e \"s/\\’/\\'/g\" \\\n",
        "                > \"$DATA/valid${j}.${SRC}-${TGT}.${LANG}\"\n",
        "        done\n",
        "    done\n",
        "done\n",
        "\n",
        "# learn BPE with sentencepiece\n",
        "TRAIN_FILES=$(for SRC in \"${SRCS[@]}\"; do echo $DATA/train.${SRC}-${TGT}.${SRC}; echo $DATA/train.${SRC}-${TGT}.${TGT}; done | tr \"\\n\" \",\")\n",
        "echo \"learning joint BPE over ${TRAIN_FILES}...\"\n",
        "python \"$SPM_TRAIN\" \\\n",
        "    --input=$TRAIN_FILES \\\n",
        "    --model_prefix=$DATA/sentencepiece.bpe \\\n",
        "    --vocab_size=$BPESIZE \\\n",
        "    --character_coverage=1.0 \\\n",
        "    --model_type=bpe\n",
        "\n",
        "# encode train/valid/test\n",
        "echo \"encoding train/valid with learned BPE...\"\n",
        "for SRC in \"${SRCS[@]}\"; do\n",
        "    for LANG in \"$SRC\" \"$TGT\"; do\n",
        "        python \"$SPM_ENCODE\" \\\n",
        "            --model \"$DATA/sentencepiece.bpe.model\" \\\n",
        "            --output_format=piece \\\n",
        "            --inputs $DATA/train.${SRC}-${TGT}.${SRC} $DATA/train.${SRC}-${TGT}.${TGT} \\\n",
        "            --outputs $DATA/train.bpe.${SRC}-${TGT}.${SRC} $DATA/train.bpe.${SRC}-${TGT}.${TGT} \\\n",
        "            --min-len $TRAIN_MINLEN --max-len $TRAIN_MAXLEN\n",
        "        python \"$SPM_ENCODE\" \\\n",
        "            --model \"$DATA/sentencepiece.bpe.model\" \\\n",
        "            --output_format=piece \\\n",
        "            --inputs $DATA/valid0.${SRC}-${TGT}.${SRC} $DATA/valid0.${SRC}-${TGT}.${TGT} \\\n",
        "            --outputs $DATA/valid0.bpe.${SRC}-${TGT}.${SRC} $DATA/valid0.bpe.${SRC}-${TGT}.${TGT}\n",
        "    done\n",
        "done"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process is interrupted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyAeAdKvC9S6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}