{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17-02fairseq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KKRpehdqxMtn0SX7j62h-8NlIFfXEeC9",
      "authorship_tag": "ABX9TyO7cDklFdxoHRsJvZxETnqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meghraj-Webllisto/language-translation/blob/master/17_02fairseq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mCRKAqzPSey",
        "colab_type": "text"
      },
      "source": [
        "# working on prepairing training data fo coustom language chinese or japense or korean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyrOZH3GPdTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af90d9a1-716f-433a-c500-e8e9bc136a1f"
      },
      "source": [
        "cd /content/drive/My Drive/fairseq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fairseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK1abx8rPocZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3f3a94ba-a27c-491f-9124-bca287b60346"
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/              \u001b[01;34mexamples\u001b[0m/          generate.py     score.py\n",
            "CODE_OF_CONDUCT.md  \u001b[01;34mfairseq\u001b[0m/           hubconf.py      \u001b[01;34mscripts\u001b[0m/\n",
            "CONTRIBUTING.md     \u001b[01;34mfairseq_cli\u001b[0m/       interactive.py  setup.py\n",
            "\u001b[01;34mdist\u001b[0m/               \u001b[01;34mfairseq.egg-info\u001b[0m/  LICENSE         \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdocs\u001b[0m/               fairseq.gif        preprocess.py   train.py\n",
            "eval_lm.py          fairseq_logo.png   README.md       validate.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMrm2vNYPpak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "728c920a-cc3c-4f94-a816-5f6d920674bf"
      },
      "source": [
        "cd scripts "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fairseq/scripts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4C6ThI9PsyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9fdc7172-1abe-417c-d12d-30805b1fe296"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average_checkpoints.py  count_docs.py        split_train_valid_docs.py\n",
            "build_sym_alignment.py  __init__.py          spm_decode.py\n",
            "compare_namespaces.py   read_binarized.py    spm_encode.py\n",
            "compound_split_bleu.sh  rm_pt.py             spm_train.py\n",
            "convert_dictionary.lua  sacrebleu_pregen.sh  wav2vec_featurize.py\n",
            "convert_model.lua       shard_docs.py        wav2vec_manifest.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQaTAfB7Pt97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eda4adea-a59d-4bc5-d10b-d8743c56006c"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/fairseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWmyhG5PUal1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8a5719c4-b9b9-400c-bab3-47b4c45b614f"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RipaNQUnXxCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rm -r custom_orig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOlq_gm8P1ef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b352962d-7d6f-4bce-c270-c6d2fb88a772"
      },
      "source": [
        "%%bash\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "SRCS=(\n",
        "    \"ja\"\n",
        ")\n",
        "TGT=en\n",
        "\n",
        "ROOT=$(dirname \"$0\")\n",
        "SCRIPTS=./scripts\n",
        "SPM_TRAIN=$SCRIPTS/spm_train.py\n",
        "SPM_ENCODE=$SCRIPTS/spm_encode.py\n",
        "\n",
        "BPESIZE=16384\n",
        "ORIG=$ROOT/custom_orig\n",
        "DATA=$ROOT/custom.ja_en.en.bpe16k\n",
        "mkdir -p \"$ORIG\" \"$DATA\"\n",
        "\n",
        "TRAIN_MINLEN=1  # remove sentences with <1 BPE token\n",
        "TRAIN_MAXLEN=250  # remove sentences with >250 BPE tokens\n",
        "\n",
        "# URLS=(\n",
        "#     \"https://wit3.fbk.eu/archive/2017-01-trnted/texts/de/en/de-en.tgz\"\n",
        "\n",
        "#     \"https://wit3.fbk.eu/archive/2017-01-trnted/texts/fr/en/fr-en.tgz\"\n",
        "# )\n",
        "# ARCHIVES=(\n",
        "#     \"de-en.tgz\"\n",
        "#     \"fr-en.tgz\"\n",
        "# )\n",
        "# VALID_SETS=(\n",
        "#     \"IWSLT17.TED.dev2010.de-en IWSLT17.TED.tst2010.de-en IWSLT17.TED.tst2011.de-en IWSLT17.TED.tst2012.de-en IWSLT17.TED.tst2013.de-en IWSLT17.TED.tst2014.de-en IWSLT17.TED.tst2015.de-en\"\n",
        "#     \"IWSLT17.TED.dev2010.fr-en IWSLT17.TED.tst2010.fr-en IWSLT17.TED.tst2011.fr-en IWSLT17.TED.tst2012.fr-en IWSLT17.TED.tst2013.fr-en IWSLT17.TED.tst2014.fr-en IWSLT17.TED.tst2015.fr-en\"\n",
        "# )\n",
        "\n",
        "# download and extract data\n",
        "# for ((i=0;i<${#URLS[@]};++i)); do\n",
        "#     ARCHIVE=$ORIG/${ARCHIVES[i]}\n",
        "#     if [ -f \"$ARCHIVE\" ]; then\n",
        "#         echo \"$ARCHIVE already exists, skipping download\"\n",
        "#     else\n",
        "#         URL=${URLS[i]}\n",
        "#         wget -P \"$ORIG\" \"$URL\"\n",
        "#         if [ -f \"$ARCHIVE\" ]; then\n",
        "#             echo \"$URL successfully downloaded.\"\n",
        "#         else\n",
        "#             echo \"$URL not successfully downloaded.\"\n",
        "#             exit 1\n",
        "#         fi\n",
        "#     fi\n",
        "#     FILE=${ARCHIVE: -4}\n",
        "#     if [ -e \"$FILE\" ]; then\n",
        "#         echo \"$FILE already exists, skipping extraction\"\n",
        "#     else\n",
        "#         tar -C \"$ORIG\" -xzvf \"$ARCHIVE\"\n",
        "#     fi\n",
        "# done\n",
        "\n",
        "echo \"pre-processing train data...\"\n",
        "\n",
        "for LANG in \"ja\" \"en\"; do\n",
        "    cat \"$ORIG/ja\"-${TGT}.${LANG} \\\n",
        "        | grep -v '<url>' \\\n",
        "        | grep -v '<talkid>' \\\n",
        "        | grep -v '<keywords>' \\\n",
        "        | grep -v '<speaker>' \\\n",
        "        | grep -v '<reviewer' \\\n",
        "        | grep -v '<translator' \\\n",
        "        | grep -v '<doc' \\\n",
        "        | grep -v '</doc>' \\\n",
        "        | sed -e 's/<title>//g' \\\n",
        "        | sed -e 's/<\\/title>//g' \\\n",
        "        | sed -e 's/<description>//g' \\\n",
        "        | sed -e 's/<\\/description>//g' \\\n",
        "        | sed 's/^\\s*//g' \\\n",
        "        | sed 's/\\s*$//g' \\\n",
        "        > \"$DATA/${SRC}-${TGT}.${LANG}\"\n",
        "done\n",
        "\n",
        "echo \"pre-processing valid data...\"\n",
        "for ((i=0;i<${#SRCS[@]};++i)); do\n",
        "    SRC=${SRCS[i]}\n",
        "    VALID_SET=(${VALID_SETS[i]})\n",
        "    for ((j=0;j<${#VALID_SET[@]};++j)); do\n",
        "        FILE=${VALID_SET[j]}\n",
        "        for LANG in \"$SRC\" \"$TGT\"; do\n",
        "            grep '<seg id' \"$ORIG/${SRC}-${TGT}/${FILE}.${LANG}.xml\" \\\n",
        "                | sed -e 's/<seg id=\"[0-9]*\">\\s*//g' \\\n",
        "                | sed -e 's/\\s*<\\/seg>\\s*//g' \\\n",
        "                | sed -e \"s/\\’/\\'/g\" \\\n",
        "                > \"$DATA/valid${j}.${SRC}-${TGT}.${LANG}\"\n",
        "        done\n",
        "    done\n",
        "done\n",
        "\n",
        "# learn BPE with sentencepiece\n",
        "TRAIN_FILES=$(for SRC in \"${SRCS[@]}\"; do echo $DATA/train.${SRC}-${TGT}.${SRC}; echo $DATA/train.${SRC}-${TGT}.${TGT}; done | tr \"\\n\" \",\")\n",
        "echo \"learning joint BPE over ${TRAIN_FILES}...\"\n",
        "python \"$SPM_TRAIN\" \\\n",
        "    --input=$TRAIN_FILES \\\n",
        "    --model_prefix=$DATA/sentencepiece.bpe \\\n",
        "    --vocab_size=$BPESIZE \\\n",
        "    --character_coverage=1.0 \\\n",
        "    --model_type=bpe\n",
        "\n",
        "# encode train/valid/test\n",
        "echo \"encoding train/valid with learned BPE...\"\n",
        "for SRC in \"${SRCS[@]}\"; do\n",
        "    for LANG in \"$SRC\" \"$TGT\"; do\n",
        "        python \"$SPM_ENCODE\" \\\n",
        "            --model \"$DATA/sentencepiece.bpe.model\" \\\n",
        "            --output_format=piece \\\n",
        "            --inputs $DATA/train.${SRC}-${TGT}.${SRC} $DATA/train.${SRC}-${TGT}.${TGT} \\\n",
        "            --outputs $DATA/train.bpe.${SRC}-${TGT}.${SRC} $DATA/train.bpe.${SRC}-${TGT}.${TGT} \\\n",
        "            --min-len $TRAIN_MINLEN --max-len $TRAIN_MAXLEN\n",
        "        python \"$SPM_ENCODE\" \\\n",
        "            --model \"$DATA/sentencepiece.bpe.model\" \\\n",
        "            --output_format=piece \\\n",
        "            --inputs $DATA/valid.${SRC}-${TGT}.${SRC} $DATA/valid.${SRC}-${TGT}.${TGT} \\\n",
        "            --outputs $DATA/valid.bpe.${SRC}-${TGT}.${SRC} $DATA/valid.bpe.${SRC}-${TGT}.${TGT}\n",
        "    done\n",
        "done"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pre-processing train data...\n",
            "pre-processing valid data...\n",
            "learning joint BPE over ./custom.ja_en.en.bpe16k/train.ja-en.ja,./custom.ja_en.en.bpe16k/train.ja-en.en,...\n",
            "encoding train/valid with learned BPE...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(116) LOG(INFO) Running command: --input=./custom.ja_en.en.bpe16k/train.ja-en.ja,./custom.ja_en.en.bpe16k/train.ja-en.en, --model_prefix=./custom.ja_en.en.bpe16k/sentencepiece.bpe --vocab_size=16384 --character_coverage=1.0 --model_type=bpe\n",
            "sentencepiece_trainer.cc(49) LOG(INFO) Starts training with : \n",
            "TrainerSpec {\n",
            "  input: ./custom.ja_en.en.bpe16k/train.ja-en.ja\n",
            "  input: ./custom.ja_en.en.bpe16k/train.ja-en.en\n",
            "  input_format: \n",
            "  model_prefix: ./custom.ja_en.en.bpe16k/sentencepiece.bpe\n",
            "  model_type: BPE\n",
            "  vocab_size: 16384\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "}\n",
            "NormalizerSpec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "\n",
            "trainer_interface.cc(267) LOG(INFO) Loading corpus: ./custom.ja_en.en.bpe16k/train.ja-en.ja\n",
            "Traceback (most recent call last):\n",
            "  File \"./scripts/spm_train.py\", line 16, in <module>\n",
            "    spm.SentencePieceTrainer.Train(\" \".join(sys.argv[1:]))\n",
            "OSError: Not found: \"./custom.ja_en.en.bpe16k/train.ja-en.ja\": No such file or directory Error #2\n",
            "Traceback (most recent call last):\n",
            "  File \"./scripts/spm_encode.py\", line 99, in <module>\n",
            "    main()\n",
            "  File \"./scripts/spm_encode.py\", line 36, in main\n",
            "    sp.Load(args.model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sentencepiece.py\", line 118, in Load\n",
            "    return _sentencepiece.SentencePieceProcessor_Load(self, filename)\n",
            "OSError: Not found: \"./custom.ja_en.en.bpe16k/sentencepiece.bpe.model\": No such file or directory Error #2\n",
            "Traceback (most recent call last):\n",
            "  File \"./scripts/spm_encode.py\", line 99, in <module>\n",
            "    main()\n",
            "  File \"./scripts/spm_encode.py\", line 36, in main\n",
            "    sp.Load(args.model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sentencepiece.py\", line 118, in Load\n",
            "    return _sentencepiece.SentencePieceProcessor_Load(self, filename)\n",
            "OSError: Not found: \"./custom.ja_en.en.bpe16k/sentencepiece.bpe.model\": No such file or directory Error #2\n",
            "Traceback (most recent call last):\n",
            "  File \"./scripts/spm_encode.py\", line 99, in <module>\n",
            "    main()\n",
            "  File \"./scripts/spm_encode.py\", line 36, in main\n",
            "    sp.Load(args.model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sentencepiece.py\", line 118, in Load\n",
            "    return _sentencepiece.SentencePieceProcessor_Load(self, filename)\n",
            "OSError: Not found: \"./custom.ja_en.en.bpe16k/sentencepiece.bpe.model\": No such file or directory Error #2\n",
            "Traceback (most recent call last):\n",
            "  File \"./scripts/spm_encode.py\", line 99, in <module>\n",
            "    main()\n",
            "  File \"./scripts/spm_encode.py\", line 36, in main\n",
            "    sp.Load(args.model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sentencepiece.py\", line 118, in Load\n",
            "    return _sentencepiece.SentencePieceProcessor_Load(self, filename)\n",
            "OSError: Not found: \"./custom.ja_en.en.bpe16k/sentencepiece.bpe.model\": No such file or directory Error #2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeE5eZXYTvbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}